{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EViPnrPitGR",
    "outputId": "d0614f8f-5450-46d4-d00f-f2d9e2dcf1e8",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install pandas pandas-ta plotly ZigZag scipy TA-Lib tensorflow scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h2Dm9p3kpLW0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 22:09:15.056744: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-17 22:09:15.056802: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import talib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj-Ayxtk9wq4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AlGqCKMLei7T",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NAME = 'Take profit and stop loss prediction'\n",
    "TIME_INTERVAL = 60\n",
    "TAKE_STEPS_TO_WATCH = 128\n",
    "INPUT_WIDTH = 512\n",
    "LSTM_INPUT_WIDTH = 512\n",
    "TAKE_PROFIT_PERCENT = 2\n",
    "STOP_LOSS_PERCENT = 1\n",
    "CURRENCY_PAIR = 'btc_usdt'\n",
    "DROPOUT = 0.01        # 0.1\n",
    "L2_REGULARIZERS = 0.002 # 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "LOOKAHEAD = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'/mnt/d/Users/Neon/PycharmProjects/tradingBot/notebooks'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PrH0opapXA7",
    "outputId": "a96c3e43-2104-42c4-b2d1-2c2274ba7dea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RATES_PATH = f\"{os.getcwd()}/../csv\"\n",
    "EXAMPLE_PATH = f\"{os.getcwd()}/checkpoints/\"\n",
    "\n",
    "if LOOKAHEAD:\n",
    "  dataleak = 'DATA LEAK '\n",
    "else:\n",
    "  dataleak = ''\n",
    "\n",
    "CHECKPOINT_PATH = f\"{EXAMPLE_PATH}/{dataleak}{NAME} {CURRENCY_PAIR.upper()} {TIME_INTERVAL} steps {TAKE_STEPS_TO_WATCH} iw {INPUT_WIDTH} lstm {LSTM_INPUT_WIDTH} tp {TAKE_PROFIT_PERCENT} sl {STOP_LOSS_PERCENT}/\"\n",
    "\n",
    "if not os.path.exists(EXAMPLE_PATH):\n",
    "  os.mkdir(EXAMPLE_PATH)\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "  os.mkdir(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SClSJNEipayw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Функция индикаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7pFng62WhkwW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def append_all_indicators(df):\n",
    "\n",
    "  df = append_candles_indicators(df)\n",
    "  df = append_overlap_indicators(df)\n",
    "  df = append_momentum_indicators(df)\n",
    "  df = append_statistics_indicators(df)\n",
    "  df = append_trend_indicators(df)\n",
    "  df = append_aberration_indicators(df)\n",
    "  df = append_volume_indicators(df)\n",
    "  \n",
    "  return df\n",
    "\n",
    "def append_candles_indicators(df):\n",
    "\n",
    "  names = [\n",
    "           \"2crows\",\n",
    "          \"3blackcrows\",\n",
    "          \"3inside\",\n",
    "          \"3linestrike\",\n",
    "          \"3outside\",\n",
    "          \"3starsinsouth\",\n",
    "          \"3whitesoldiers\",\n",
    "          \"abandonedbaby\",\n",
    "          \"advanceblock\",\n",
    "          \"belthold\",\n",
    "          \"breakaway\",\n",
    "          \"closingmarubozu\",\n",
    "          \"concealbabyswall\",\n",
    "          \"counterattack\",\n",
    "          \"darkcloudcover\",\n",
    "          \"doji\",\n",
    "          \"dojistar\",\n",
    "          \"dragonflydoji\",\n",
    "          \"engulfing\",\n",
    "          \"eveningdojistar\",\n",
    "          \"eveningstar\",\n",
    "          \"gapsidesidewhite\",\n",
    "          \"gravestonedoji\",\n",
    "          \"hammer\",\n",
    "          \"hangingman\",\n",
    "          \"harami\",\n",
    "          \"haramicross\",\n",
    "          \"highwave\",\n",
    "          \"hikkake\",\n",
    "          \"hikkakemod\",\n",
    "          \"homingpigeon\",\n",
    "          \"identical3crows\",\n",
    "          \"inneck\",\n",
    "          \"inside\",\n",
    "          \"invertedhammer\",\n",
    "          \"kicking\",\n",
    "          \"kickingbylength\",\n",
    "          \"ladderbottom\",\n",
    "          \"longleggeddoji\",\n",
    "          \"longline\",\n",
    "          \"marubozu\",\n",
    "          \"matchinglow\",\n",
    "          \"mathold\",\n",
    "          \"morningdojistar\",\n",
    "          \"morningstar\",\n",
    "          \"onneck\",\n",
    "          \"piercing\",\n",
    "          \"rickshawman\",\n",
    "          \"risefall3methods\",\n",
    "          \"separatinglines\",\n",
    "          \"shootingstar\",\n",
    "          \"shortline\",\n",
    "          \"spinningtop\",\n",
    "          \"stalledpattern\",\n",
    "          \"sticksandwich\",\n",
    "          \"takuri\",\n",
    "          \"tasukigap\",\n",
    "          \"thrusting\",\n",
    "          \"tristar\",\n",
    "          \"unique3river\",\n",
    "          \"upsidegap2crows\",\n",
    "          \"xsidegap3methods\"\n",
    "  ]\n",
    "\n",
    "  df.ta.cdl_pattern(name=names, append=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def append_momentum_indicators(df):\n",
    "\n",
    "  df.ta.apo(append=True)\n",
    "  df.ta.bias(append=True)\n",
    "  df.ta.bop(append=True)\n",
    "  df.ta.brar(append=True)\n",
    "  df.ta.cci(append=True)\n",
    "  df.ta.cfo(append=True)\n",
    "  df.ta.cg(append=True)\n",
    "  df.ta.cmo(append=True)\n",
    "  df.ta.coppock(append=True)\n",
    "  df.ta.cti(append=True)\n",
    "  df.ta.dm(append=True)\n",
    "  df.ta.er(append=True)\n",
    "  df.ta.eri(append=True)\n",
    "  df.ta.fisher(append=True)\n",
    "  df.ta.inertia(append=True)\n",
    "  df.ta.kdj(append=True)\n",
    "  df.ta.kst(append=True)\n",
    "  df.ta.macd(append=True, )\n",
    "  df.ta.mom(append=True)\n",
    "  df.ta.pgo(append=True)\n",
    "  df.ta.ppo(append=True)\n",
    "  df.ta.psl(append=True)\n",
    "  df.ta.pvo(append=True)\n",
    "  df.ta.qqe(append=True)\n",
    "  df.ta.roc(append=True)\n",
    "  df.ta.rsi(append=True)\n",
    "  df.ta.rsx(append=True)\n",
    "  df.ta.rvgi(append=True)\n",
    "  # df.ta.stc(append=True) вываливается ошибка\n",
    "  df.ta.slope(append=True)\n",
    "  df.ta.smi(append=True)\n",
    "  df.ta.squeeze(append=True) \n",
    "  df.ta.squeeze_pro(append=True) \n",
    "  df.ta.stoch(append=True)\n",
    "  df.ta.stochrsi(append=True)\n",
    "  #df.ta.td_seq(append=True)   # медленный\n",
    "  df['TD_SEQ_UP'] = 0\n",
    "  df['TD_SEQ_DN'] = 0\n",
    "  df.ta.trix(append=True)\n",
    "  df.ta.tsi(append=True)\n",
    "  df.ta.uo(append=True)\n",
    "  df.ta.willr(append=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def append_overlap_indicators(df):\n",
    "\n",
    "  df.ta.alma(append=True)\n",
    "  df.ta.dema(append=True)\n",
    "  df.ta.ema(append=True)\n",
    "  df.ta.fwma(append=True)\n",
    "  df.ta.hilo(append=True)\n",
    "  df.ta.hl2(append=True)\n",
    "  df.ta.hlc3(append=True)\n",
    "  df.ta.hma(append=True)\n",
    "  df.ta.hwma(append=True)\n",
    "  df.ta.ichimoku(append=True, lookahead=LOOKAHEAD) # data leak\n",
    "  #df.ta.jma(append=True)\n",
    "  df.ta.kama(append=True)\n",
    "  df.ta.linreg(append=True)\n",
    "  df.ta.mcgd(append=True)\n",
    "  df.ta.midpoint(append=True)\n",
    "  df.ta.midprice(append=True)\n",
    "  df.ta.ohlc4(append=True)\n",
    "  df.ta.pwma(append=True)\n",
    "  df.ta.rma(append=True)\n",
    "  df.ta.sinwma(append=True)\n",
    "  df.ta.sma(append=True)\n",
    "  df.ta.ssf(append=True)\n",
    "  df.ta.supertrend(append=True)\n",
    "  df.ta.swma(append=True)\n",
    "  df.ta.t3(append=True)\n",
    "  df.ta.tema(append=True)\n",
    "  df.ta.trima(append=True)\n",
    "  df.ta.vidya(append=True)\n",
    "  #df.ta.vwap(append=True) ошибка\n",
    "  df.ta.vwma(append=True)\n",
    "  df.ta.wcp(append=True) \n",
    "  df.ta.wma(append=True) \n",
    "  df.ta.zlma(append=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def append_statistics_indicators(df):\n",
    "  df.ta.entropy(append=True)\n",
    "  df.ta.kurtosis(append=True)\n",
    "  df.ta.mad(append=True)\n",
    "  df.ta.median(append=True)\n",
    "  df.ta.quantile(append=True)\n",
    "  df.ta.skew(append=True)\n",
    "  df.ta.stdev(append=True)\n",
    "\n",
    "  if LOOKAHEAD:\n",
    "    df.ta.tos_stdevall(append=True) #возможная утечка данных в будущее\n",
    "  else:\n",
    "    df['TOS_STDEVALL_LR'] = 0\n",
    "    df['TOS_STDEVALL_L_1'] = 0\n",
    "    df['TOS_STDEVALL_U_1'] = 0\n",
    "    df['TOS_STDEVALL_L_2'] = 0\n",
    "    df['TOS_STDEVALL_U_2'] = 0\n",
    "    df['TOS_STDEVALL_L_3'] = 0\n",
    "    df['TOS_STDEVALL_U_3'] = 0\n",
    "\n",
    "  df.ta.variance(append=True)\n",
    "  df.ta.zscore(append=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def append_trend_indicators(df):\n",
    "  df.ta.adx(append=True)\n",
    "  df.ta.amat(append=True)\n",
    "  df.ta.aroon(append=True)\n",
    "  df.ta.chop(append=True)\n",
    "  df.ta.cksp(append=True)\n",
    "  #df.ta.decay(append=True) ошибка\n",
    "  df.ta.decreasing(append=True)\n",
    "  df.ta.dpo(append=True, lookahead=LOOKAHEAD) # data leak\n",
    "  df.ta.increasing(append=True)\n",
    "  df.ta.long_run(append=True)\n",
    "  df.ta.psar(append=True)\n",
    "  df.ta.qstick(append=True)\n",
    "  df.ta.short_run(append=True)\n",
    "  df.ta.tsignals(append=True)\n",
    "  df.ta.ttm_trend(append=True)\n",
    "  df.ta.vhf(append=True)\n",
    "  df.ta.vortex(append=True)\n",
    "  df.ta.xsignals(append=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def append_utility_indicators(df):\n",
    "  # не работают\n",
    "  df.ta.above(append=True)\n",
    "  df.ta.above_value(append=True)\n",
    "  df.ta.below(append=True)\n",
    "  df.ta.below_value(append=True)\n",
    "  df.ta.cross(append=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def append_aberration_indicators(df):\n",
    "  df.ta.aberration(append=True)\n",
    "  df.ta.accbands(append=True)\n",
    "  df.ta.atr(append=True)\n",
    "  df.ta.bbands(append=True)\n",
    "  df.ta.donchian(append=True)\n",
    "  #df.ta.hwc(append=True) ошибка\n",
    "  df.ta.kc(append=True)\n",
    "  df.ta.massi(append=True)\n",
    "  df.ta.natr(append=True)\n",
    "  df.ta.pdist(append=True)\n",
    "  df.ta.rvi(append=True)\n",
    "  df.ta.thermo(append=True)\n",
    "  df.ta.true_range(append=True)\n",
    "  df.ta.ui(append=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def append_volume_indicators(df):\n",
    "  df.ta.ad(append=True)\n",
    "  df.ta.adosc(append=True)\n",
    "  df.ta.aobv(append=True)\n",
    "  df.ta.cmf(append=True)\n",
    "  df.ta.efi(append=True)\n",
    "  df.ta.eom(append=True)\n",
    "  df.ta.kvo(append=True)\n",
    "  df.ta.mfi(append=True)\n",
    "  df.ta.nvi(append=True)\n",
    "  df.ta.obv(append=True)\n",
    "  df.ta.pvi(append=True)\n",
    "  df.ta.pvol(append=True)\n",
    "  df.ta.pvr(append=True)\n",
    "  df.ta.pvt(append=True)\n",
    "  df.ta.vp(append=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def append_cycles(df):\n",
    "  #df.ta.ebsw(append=True) появляется ошибка \n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x8W2efycINfP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def append_indicators(df):\n",
    "\n",
    "  #df.ta.macd(append=True, fast=12, slow=26, signal=9)\n",
    "  #df.ta.macd(append=True, fast=8, slow=17, signal=9)\n",
    "  #df.ta.ema(append=True, length=200)\n",
    "  #df.ta.stoch(append=True, k=14, d=3)\n",
    "  #df.ta.rsi(append=True, length=14)\n",
    "\n",
    "  df = append_all_indicators(df)\n",
    "\n",
    "  #for length in range(5, 200):\n",
    "  #  df.ta.ema(append=True, length=length)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdDeR-VtrjOf",
    "outputId": "3036db70-6edc-4a29-e415-42d7c26884d8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_name = f'{CURRENCY_PAIR}_{TIME_INTERVAL}.csv.zip'\n",
    "\n",
    "file_path = f'file://{RATES_PATH}/{file_name}'\n",
    "\n",
    "df = pd.read_csv(file_path, decimal='.', keep_default_na=False, encoding = \"UTF-8\", compression='zip')\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "df = (df.replace((np.inf, -np.inf), np.nan)\n",
    "                     .dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df.tail(700_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QHqsoesS3rgP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def processing(df, row, take_steps_to_watch):\n",
    "\n",
    "  index = row.name\n",
    "\n",
    "  loc = df.index.get_loc(index) + 1\n",
    "\n",
    "  sample = df.iloc[loc:loc + take_steps_to_watch]\n",
    "\n",
    "  position = 0\n",
    "\n",
    "  for index, sample_row in sample.iterrows():\n",
    "    if sample_row['high'] >= row['long_take_profit_price']:\n",
    "      return 1\n",
    "    if sample_row['low'] <= row['long_stop_loss_price']:\n",
    "      return 0\n",
    "      \n",
    "  return position\n",
    "\n",
    "def append_position(original_df):\n",
    "\n",
    "  df = original_df.copy()\n",
    "\n",
    "  df['long_take_profit_price'] = (100 + TAKE_PROFIT_PERCENT) * df['close'] / 100\n",
    "  df['long_stop_loss_price'] = (100 - STOP_LOSS_PERCENT) * df['close'] / 100\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "    df.at[index, 'position'] = processing(df, row, TAKE_STEPS_TO_WATCH)\n",
    "\n",
    "  original_df['position'] = df['position']\n",
    "\n",
    "  return original_df\n",
    "\n",
    "df = append_position(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S5QWK_fG0RG-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def append_position(original_df):\n",
    "\n",
    "  df = original_df.copy()\n",
    "\n",
    "  df[f'future_high_{TAKE_STEPS_TO_WATCH}'] = df['high'].rolling(TAKE_STEPS_TO_WATCH).max().shift(-TAKE_STEPS_TO_WATCH)\n",
    "  df[f'future_low_{TAKE_STEPS_TO_WATCH}'] = df['low'].rolling(TAKE_STEPS_TO_WATCH).min().shift(-TAKE_STEPS_TO_WATCH)\n",
    "\n",
    "  df[f'future_high_{TAKE_STEPS_TO_WATCH}_percentage'] = (df[f'future_high_{TAKE_STEPS_TO_WATCH}'] * 100 / df['close']) - 100\n",
    "  df[f'future_low_{TAKE_STEPS_TO_WATCH}_percentage'] = 100 - (df[f'future_low_{TAKE_STEPS_TO_WATCH}'] * 100 / df['close'])\n",
    "\n",
    "  df['long_position'] = 0\n",
    "\n",
    "  df.loc[(df[f'future_high_{TAKE_STEPS_TO_WATCH}_percentage'] > TAKE_PROFIT_PERCENT) & (df[f'future_low_{TAKE_STEPS_TO_WATCH}_percentage'] < STOP_LOSS_PERCENT), 'long_position'] = 1\n",
    "\n",
    "  df['short_position'] = 0\n",
    "\n",
    "  df.loc[(df[f'future_high_{TAKE_STEPS_TO_WATCH}_percentage'] < STOP_LOSS_PERCENT) & (df[f'future_low_{TAKE_STEPS_TO_WATCH}_percentage'] > TAKE_PROFIT_PERCENT), 'short_position'] = 1\n",
    "\n",
    "  df['position'] = 0\n",
    "\n",
    "  df.loc[df['long_position'] > 0, 'position'] = 1\n",
    "  df.loc[df['short_position'] > 0, 'position'] = -1\n",
    "\n",
    "  original_df['position'] = df['position']\n",
    "\n",
    "  return original_df\n",
    "\n",
    "#df = append_position(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdoSQg9EIT85",
    "outputId": "041f5261-0d47-4a68-e736-2ba7888c17a2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long trend count: 46804\n",
      "Neutral trend count: 653196\n",
      "Short trend count: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Long trend count:\", len(df[df['position'] > 0]))\n",
    "print(\"Neutral trend count:\", len(df[df['position'] == 0]))\n",
    "print(\"Short trend count:\", len(df[df['position'] < 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6QrAbL2Whs7G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/pandas_ta/overlap/linreg.py:52: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  rd = (divisor * (length * y2_sum - y_sum * y_sum)) ** 0.5\n",
      "/home/user/.local/lib/python3.10/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = append_indicators(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XizdWj5Ccp-o",
    "outputId": "d8e1fff6-7cbc-44ed-ae97-d52fbb1cfcf0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['datetime', 'CDL_2CROWS', 'CDL_3BLACKCROWS', 'CDL_3INSIDE',\n       'CDL_3LINESTRIKE', 'CDL_3OUTSIDE', 'CDL_3STARSINSOUTH',\n       'CDL_3WHITESOLDIERS', 'CDL_ABANDONEDBABY', 'CDL_ADVANCEBLOCK',\n       'CDL_BELTHOLD', 'CDL_BREAKAWAY', 'CDL_CLOSINGMARUBOZU',\n       'CDL_CONCEALBABYSWALL', 'CDL_COUNTERATTACK', 'CDL_DARKCLOUDCOVER',\n       'CDL_DOJI_10_0.1', 'CDL_DOJISTAR', 'CDL_DRAGONFLYDOJI',\n       'CDL_ENGULFING', 'CDL_EVENINGDOJISTAR', 'CDL_EVENINGSTAR',\n       'CDL_GAPSIDESIDEWHITE', 'CDL_GRAVESTONEDOJI', 'CDL_HAMMER',\n       'CDL_HANGINGMAN', 'CDL_HARAMI', 'CDL_HARAMICROSS', 'CDL_HIGHWAVE',\n       'CDL_HIKKAKE', 'CDL_HIKKAKEMOD', 'CDL_HOMINGPIGEON',\n       'CDL_IDENTICAL3CROWS', 'CDL_INNECK', 'CDL_INSIDE',\n       'CDL_INVERTEDHAMMER', 'CDL_KICKING', 'CDL_KICKINGBYLENGTH',\n       'CDL_LADDERBOTTOM', 'CDL_LONGLEGGEDDOJI', 'CDL_LONGLINE',\n       'CDL_MARUBOZU', 'CDL_MATCHINGLOW', 'CDL_MATHOLD',\n       'CDL_MORNINGDOJISTAR', 'CDL_MORNINGSTAR', 'CDL_ONNECK',\n       'CDL_PIERCING', 'CDL_RICKSHAWMAN', 'CDL_RISEFALL3METHODS',\n       'CDL_SEPARATINGLINES', 'CDL_SHOOTINGSTAR', 'CDL_SHORTLINE',\n       'CDL_SPINNINGTOP', 'CDL_STALLEDPATTERN', 'CDL_STICKSANDWICH',\n       'CDL_TAKURI', 'CDL_TASUKIGAP', 'CDL_THRUSTING', 'CDL_TRISTAR',\n       'CDL_UNIQUE3RIVER', 'CDL_UPSIDEGAP2CROWS', 'CDL_XSIDEGAP3METHODS',\n       'ALMA_10_6.0_0.85', 'DEMA_10', 'EMA_10', 'FWMA_10', 'HILO_13_21',\n       'HILOl_13_21', 'HILOs_13_21', 'HL2', 'HLC3', 'HMA_10',\n       'HWMA_0.2_0.1_0.1', 'ISA_9', 'ISB_26', 'ITS_9', 'IKS_26',\n       'KAMA_10_2_30', 'LR_14', 'MCGD_10', 'MIDPOINT_2', 'MIDPRICE_2',\n       'OHLC4', 'PWMA_10', 'RMA_10', 'SINWMA_14', 'SMA_10', 'SSF_10_2',\n       'SUPERT_7_3.0', 'SUPERTd_7_3.0', 'SUPERTl_7_3.0', 'SUPERTs_7_3.0',\n       'SWMA_10', 'T3_10_0.7', 'TEMA_10', 'TRIMA_10', 'VIDYA_14',\n       'VWMA_10', 'WCP', 'WMA_10', 'ZL_EMA_10', 'APO_12_26',\n       'BIAS_SMA_26', 'BOP', 'AR_26', 'BR_26', 'CCI_14_0.015', 'CFO_9',\n       'CG_10', 'CMO_14', 'COPC_11_14_10', 'CTI_12', 'DMP_14', 'DMN_14',\n       'ER_10', 'BULLP_13', 'BEARP_13', 'FISHERT_9_1', 'FISHERTs_9_1',\n       'INERTIA_20_14', 'K_9_3', 'D_9_3', 'J_9_3',\n       'KST_10_15_20_30_10_10_10_15', 'KSTs_9', 'MACD_12_26_9',\n       'MACDh_12_26_9', 'MACDs_12_26_9', 'MOM_10', 'PGO_14',\n       'PPO_12_26_9', 'PPOh_12_26_9', 'PPOs_12_26_9', 'PSL_12',\n       'PVO_12_26_9', 'PVOh_12_26_9', 'PVOs_12_26_9', 'QQE_14_5_4.236',\n       'QQE_14_5_4.236_RSIMA', 'QQEl_14_5_4.236', 'QQEs_14_5_4.236',\n       'ROC_10', 'RSI_14', 'RSX_14', 'RVGI_14_4', 'RVGIs_14_4', 'SLOPE_1',\n       'SMI_5_20_5', 'SMIs_5_20_5', 'SMIo_5_20_5', 'SQZ_20_2.0_20_1.5',\n       'SQZ_ON', 'SQZ_OFF', 'SQZ_NO', 'SQZPRO_20_2.0_20_2_1.5_1',\n       'SQZPRO_ON_WIDE', 'SQZPRO_ON_NORMAL', 'SQZPRO_ON_NARROW',\n       'SQZPRO_OFF', 'SQZPRO_NO', 'STOCHk_14_3_3', 'STOCHd_14_3_3',\n       'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3', 'TD_SEQ_UP',\n       'TD_SEQ_DN', 'TRIX_30_9', 'TRIXs_30_9', 'TSI_13_25_13',\n       'TSIs_13_25_13', 'UO_7_14_28', 'WILLR_14', 'ENTP_10', 'KURT_30',\n       'MAD_30', 'MEDIAN_30', 'QTL_30_0.5', 'SKEW_30', 'STDEV_30',\n       'TOS_STDEVALL_LR', 'TOS_STDEVALL_L_1', 'TOS_STDEVALL_U_1',\n       'TOS_STDEVALL_L_2', 'TOS_STDEVALL_U_2', 'TOS_STDEVALL_L_3',\n       'TOS_STDEVALL_U_3', 'VAR_30', 'ZS_30', 'ADX_14', 'AMATe_LR_8_21_2',\n       'AMATe_SR_8_21_2', 'AROOND_14', 'AROONU_14', 'AROONOSC_14',\n       'CHOP_14_1_100', 'CKSPl_10_3_20', 'CKSPs_10_3_20', 'DEC_1',\n       'DPO_20', 'INC_1', 'PSARl_0.02_0.2', 'PSARs_0.02_0.2',\n       'PSARaf_0.02_0.2', 'PSARr_0.02_0.2', 'QS_10', 'TTM_TRND_6',\n       'VHF_28', 'VTXP_14', 'VTXM_14', 'ABER_ZG_5_15', 'ABER_SG_5_15',\n       'ABER_XG_5_15', 'ABER_ATR_5_15', 'ACCBL_20', 'ACCBM_20',\n       'ACCBU_20', 'ATRr_14', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0',\n       'BBB_5_2.0', 'BBP_5_2.0', 'DCL_20_20', 'DCM_20_20', 'DCU_20_20',\n       'KCLe_20_2', 'KCBe_20_2', 'KCUe_20_2', 'MASSI_9_25', 'NATR_14',\n       'PDIST', 'RVI_14', 'THERMO_20_2_0.5', 'THERMOma_20_2_0.5',\n       'THERMOl_20_2_0.5', 'THERMOs_20_2_0.5', 'TRUERANGE_1', 'UI_14',\n       'AD', 'ADOSC_3_10', 'OBV', 'OBV_min_2', 'OBV_max_2', 'OBVe_4',\n       'OBVe_12', 'AOBV_LR_2', 'AOBV_SR_2', 'CMF_20', 'EFI_13',\n       'EOM_14_100000000', 'KVO_34_55_13', 'KVOs_34_55_13', 'MFI_14',\n       'NVI_1', 'PVI_1', 'PVOL', 'PVR', 'PVT', 'low_close', 'mean_close',\n       'high_close', 'pos_volume', 'neg_volume', 'total_volume'],\n      dtype=object)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = df.columns\n",
    "\n",
    "features_names_with_position = index.delete([\n",
    "                                             index.get_loc('volume'), \n",
    "                                             index.get_loc('close'),\n",
    "                                             index.get_loc('open'), \n",
    "                                             index.get_loc('high'), \n",
    "                                             index.get_loc('low'),\n",
    "                                             ])\n",
    "\n",
    "FEATURES_NAMES = features_names_with_position.delete([features_names_with_position.get_loc('position')]).values\n",
    "\n",
    "FEATURES_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Sno5eQ3QgcKd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = df.loc[:,features_names_with_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "max_val_df_length = 10000\n",
    "max_eval_df_length = 10000\n",
    "val_length = int(n*0.9) - int(n*0.8)\n",
    "eval_length = int(n*1) - int(n*0.9)\n",
    "\n",
    "if val_length > max_val_df_length:\n",
    "    val_length = max_val_df_length\n",
    "    \n",
    "if eval_length > max_eval_df_length:\n",
    "    eval_length = max_eval_df_length\n",
    "\n",
    "val_start_index = n - eval_length - val_length\n",
    "test_start_index = n - eval_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700000 680000 690000\n"
     ]
    }
   ],
   "source": [
    "print(n, val_start_index, test_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNcqnMrogkv7",
    "outputId": "a7ed430c-7d35-4d2c-ca49-47026dcace3f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = data[0:val_start_index]\n",
    "val_df = data[val_start_index:test_start_index]\n",
    "test_df = data[test_start_index:]\n",
    "\n",
    "test_full_df = df[test_start_index:]\n",
    "\n",
    "num_features = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700000 680000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(data), len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Oiad03Ja3LPH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalization(dataframe, train_mean, train_std):\n",
    "  position = dataframe.pop('position')\n",
    "  dataframe = (dataframe - train_mean) / train_std\n",
    "  dataframe['position'] = position\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzjrB6DCgtsK",
    "outputId": "2aecd1ee-f74d-404f-a7b4-35832f8af872",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16920/1147297737.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  train_mean = train_df.mean()\n",
      "/tmp/ipykernel_16920/1147297737.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  train_std = train_df.std()\n"
     ]
    }
   ],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = normalization(train_df, train_mean, train_std)\n",
    "val_df = normalization(val_df, train_mean, train_std)\n",
    "test_df = normalization(test_df, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7801Z5WwgwDi",
    "outputId": "2c6ebe1d-bde6-401c-ed06-d3cb4d4ab527",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position           0.067512\n",
      "CDL_2CROWS        -0.023676\n",
      "CDL_3BLACKCROWS   -0.069706\n",
      "CDL_3INSIDE        0.006324\n",
      "CDL_3LINESTRIKE   -0.010294\n",
      "                     ...   \n",
      "mean_close              NaN\n",
      "high_close              NaN\n",
      "pos_volume              NaN\n",
      "neg_volume              NaN\n",
      "total_volume            NaN\n",
      "Length: 265, dtype: float64 position            0.250906\n",
      "CDL_2CROWS          1.538535\n",
      "CDL_3BLACKCROWS     2.639269\n",
      "CDL_3INSIDE        10.205973\n",
      "CDL_3LINESTRIKE     5.557184\n",
      "                     ...    \n",
      "mean_close               NaN\n",
      "high_close               NaN\n",
      "pos_volume               NaN\n",
      "neg_volume               NaN\n",
      "total_volume             NaN\n",
      "Length: 265, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u492P34ygy-S",
    "outputId": "86be81f3-9340-465a-d879-6eacb9285357",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "vB4ypH2Hjbht",
    "outputId": "a0522a28-aa8f-48f1-a0f2-b32dd7b9f7e7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [ABER_ATR_5_15, ABER_SG_5_15, ABER_XG_5_15, ABER_ZG_5_15, ACCBL_20, ACCBM_20, ACCBU_20, AD, ADOSC_3_10, ADX_14, ALMA_10_6.0_0.85, AMATe_LR_8_21_2, AMATe_SR_8_21_2, AOBV_LR_2, AOBV_SR_2, APO_12_26, AROOND_14, AROONOSC_14, AROONU_14, AR_26, ATRr_14, BBB_5_2.0, BBL_5_2.0, BBM_5_2.0, BBP_5_2.0, BBU_5_2.0, BEARP_13, BIAS_SMA_26, BOP, BR_26, BULLP_13, CCI_14_0.015, CDL_2CROWS, CDL_3BLACKCROWS, CDL_3INSIDE, CDL_3LINESTRIKE, CDL_3OUTSIDE, CDL_3STARSINSOUTH, CDL_3WHITESOLDIERS, CDL_ABANDONEDBABY, CDL_ADVANCEBLOCK, CDL_BELTHOLD, CDL_BREAKAWAY, CDL_CLOSINGMARUBOZU, CDL_CONCEALBABYSWALL, CDL_COUNTERATTACK, CDL_DARKCLOUDCOVER, CDL_DOJISTAR, CDL_DOJI_10_0.1, CDL_DRAGONFLYDOJI, CDL_ENGULFING, CDL_EVENINGDOJISTAR, CDL_EVENINGSTAR, CDL_GAPSIDESIDEWHITE, CDL_GRAVESTONEDOJI, CDL_HAMMER, CDL_HANGINGMAN, CDL_HARAMI, CDL_HARAMICROSS, CDL_HIGHWAVE, CDL_HIKKAKE, CDL_HIKKAKEMOD, CDL_HOMINGPIGEON, CDL_IDENTICAL3CROWS, CDL_INNECK, CDL_INSIDE, CDL_INVERTEDHAMMER, CDL_KICKING, CDL_KICKINGBYLENGTH, CDL_LADDERBOTTOM, CDL_LONGLEGGEDDOJI, CDL_LONGLINE, CDL_MARUBOZU, CDL_MATCHINGLOW, CDL_MATHOLD, CDL_MORNINGDOJISTAR, CDL_MORNINGSTAR, CDL_ONNECK, CDL_PIERCING, CDL_RICKSHAWMAN, CDL_RISEFALL3METHODS, CDL_SEPARATINGLINES, CDL_SHOOTINGSTAR, CDL_SHORTLINE, CDL_SPINNINGTOP, CDL_STALLEDPATTERN, CDL_STICKSANDWICH, CDL_TAKURI, CDL_TASUKIGAP, CDL_THRUSTING, CDL_TRISTAR, CDL_UNIQUE3RIVER, CDL_UPSIDEGAP2CROWS, CDL_XSIDEGAP3METHODS, CFO_9, CG_10, CHOP_14_1_100, CKSPl_10_3_20, CKSPs_10_3_20, CMF_20, ...]\nIndex: []\n\n[0 rows x 266 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ABER_ATR_5_15</th>\n      <th>ABER_SG_5_15</th>\n      <th>ABER_XG_5_15</th>\n      <th>ABER_ZG_5_15</th>\n      <th>ACCBL_20</th>\n      <th>ACCBM_20</th>\n      <th>ACCBU_20</th>\n      <th>AD</th>\n      <th>ADOSC_3_10</th>\n      <th>ADX_14</th>\n      <th>...</th>\n      <th>ZL_EMA_10</th>\n      <th>ZS_30</th>\n      <th>datetime</th>\n      <th>high_close</th>\n      <th>low_close</th>\n      <th>mean_close</th>\n      <th>neg_volume</th>\n      <th>pos_volume</th>\n      <th>position</th>\n      <th>total_volume</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 266 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.fillna(0)\n",
    "val_df = val_df.fillna(0)\n",
    "test_df = test_df.fillna(0)\n",
    "\n",
    "train_df[train_df.isnull().any(axis=1)]\n",
    "train_df[train_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "f9ARLeUAg2ua",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert train_df.isnull().values.any() == False, 'Тренировочный датафрейм не должен содержать nan значений'\n",
    "assert val_df.isnull().values.any() == False, 'Датафрейм валидации не должен содержать nan значений'\n",
    "assert test_df.isnull().values.any() == False, 'Тестовый датафрейм не должен содержать nan значений'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IDwhAGpWceQJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None, input_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    self.input_columns = input_columns\n",
    "\n",
    "    self.input_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(input_columns)}   \n",
    "\n",
    "    self.label_columns = label_columns\n",
    "\n",
    "    self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}                       \n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Column indices: {self.column_indices}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}',\n",
    "        f'Label start: {self.label_start}',\n",
    "        f'Input width: {self.input_width}',\n",
    "        f'Label width: {self.label_width}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "JaaeyIqnciCo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset(self, data, shuffle=True, batch_size=32,):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "\n",
    "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "38kyb3tPckMp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "la6ecAfPcgq4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "\n",
    "  inputs = tf.stack(\n",
    "        [inputs[:, :, self.column_indices[name]] for name in self.input_columns],\n",
    "        axis=-1)\n",
    "  \n",
    "  labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "iUY869JCcgw3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "  def __init__(self, model, path, max_epochs=30, patience=3):\n",
    "    self.model = model\n",
    "    self.path = path\n",
    "    self.max_epochs = max_epochs\n",
    "    self.patience = patience\n",
    "\n",
    "  def compile(self):\n",
    "    self.model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.optimizers.Adam(learning_rate=0.0005),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  def load_weights(self):\n",
    "    self.model.load_weights(self.path)\n",
    "\n",
    "  def fit(self, window):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=self.patience,\n",
    "                                                      mode='min')\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "\n",
    "    history = self.model.fit(window.train, epochs=self.max_epochs,\n",
    "                        validation_data=window.val,\n",
    "                        callbacks=[early_stopping, cp_callback])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FV4lB3DXcifH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wide_window = WindowGenerator(\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    input_width=INPUT_WIDTH, label_width=1, shift=1,\n",
    "    label_columns=['position'], input_columns=FEATURES_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zj4BOQBYhRXI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 23:07:54.846997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-17 23:07:54.847049: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-17 23:07:54.847069: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NEON-WORKPC): /proc/driver/nvidia/version does not exist\n",
      "2022-06-17 23:07:54.847298: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(LSTM_INPUT_WIDTH, activation='tanh', return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(L2_REGULARIZERS)),\n",
    "    tf.keras.layers.Dropout(DROPOUT),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQIR9s1MpgMp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Тренировка нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "_tI8qaAXmGcS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(lstm_model, CHECKPOINT_PATH, max_epochs=20, patience=3)\n",
    "model.compile()\n",
    "\n",
    "if os.path.exists(f'{CHECKPOINT_PATH}/checkpoint'):\n",
    "  model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOYH_oU4AUnr",
    "outputId": "10203c13-92e6-4956-8154-ca12fbeb8d82",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21234/21234 [==============================] - ETA: 0s - loss: 0.1001 - mean_absolute_error: 0.1200\n",
      "Epoch 1: saving model to /mnt/d/Users/Neon/PycharmProjects/tradingBot/notebooks/checkpoints//Take profit and stop loss prediction BTC_USDT 60 steps 128 iw 512 lstm 512 tp 2 sl 1/\n",
      "21234/21234 [==============================] - 47774s 2s/step - loss: 0.1001 - mean_absolute_error: 0.1200 - val_loss: 0.0356 - val_mean_absolute_error: 0.0726\n",
      "Epoch 2/20\n",
      "21234/21234 [==============================] - ETA: 0s - loss: 0.1062 - mean_absolute_error: 0.1197\n",
      "Epoch 2: saving model to /mnt/d/Users/Neon/PycharmProjects/tradingBot/notebooks/checkpoints//Take profit and stop loss prediction BTC_USDT 60 steps 128 iw 512 lstm 512 tp 2 sl 1/\n",
      "21234/21234 [==============================] - 46307s 2s/step - loss: 0.1062 - mean_absolute_error: 0.1197 - val_loss: 0.0355 - val_mean_absolute_error: 0.0803\n",
      "Epoch 3/20\n",
      "21234/21234 [==============================] - ETA: 0s - loss: 0.0603 - mean_absolute_error: 0.1186\n",
      "Epoch 3: saving model to /mnt/d/Users/Neon/PycharmProjects/tradingBot/notebooks/checkpoints//Take profit and stop loss prediction BTC_USDT 60 steps 128 iw 512 lstm 512 tp 2 sl 1/\n",
      "21234/21234 [==============================] - 47012s 2s/step - loss: 0.0603 - mean_absolute_error: 0.1186 - val_loss: 0.0356 - val_mean_absolute_error: 0.0789\n",
      "Epoch 4/20\n",
      "21234/21234 [==============================] - ETA: 0s - loss: 0.0642 - mean_absolute_error: 0.1184\n",
      "Epoch 4: saving model to /mnt/d/Users/Neon/PycharmProjects/tradingBot/notebooks/checkpoints//Take profit and stop loss prediction BTC_USDT 60 steps 128 iw 512 lstm 512 tp 2 sl 1/\n",
      "21234/21234 [==============================] - 47943s 2s/step - loss: 0.0642 - mean_absolute_error: 0.1184 - val_loss: 0.0356 - val_mean_absolute_error: 0.0719\n",
      "Epoch 5/20\n",
      "  321/21234 [..............................] - ETA: 13:35:12 - loss: 0.0596 - mean_absolute_error: 0.1171"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [51]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwide_window\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, window)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Create a callback that saves the model's weights\u001B[39;00m\n\u001B[1;32m     22\u001B[0m cp_callback \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(filepath\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath,\n\u001B[1;32m     23\u001B[0m                                                 save_weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     24\u001B[0m                                                 verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcp_callback\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m history\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m   2451\u001B[0m   (graph_function,\n\u001B[1;32m   2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m-> 2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1856\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1859\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1860\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1861\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1862\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1863\u001B[0m     args,\n\u001B[1;32m   1864\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1865\u001B[0m     executing_eagerly)\n\u001B[1;32m   1866\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    496\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 497\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    503\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    504\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    505\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    506\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    509\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    510\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(wide_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "29HDmO6QH7SH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@property\n",
    "def test_without_shuffle(self):\n",
    "\n",
    "  return self.make_dataset(self.test_df, shuffle=False)\n",
    "\n",
    "WindowGenerator.test_without_shuffle = test_without_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ZthrB3EB0td6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "iXwTd2Gshwta",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/297 [==============================] - 271s 907ms/step - loss: 0.0315 - mean_absolute_error: 0.0979\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.03145810216665268, 0.09792950004339218]"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(wide_window.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "AvkBGld3INOX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = wide_window.test_without_shuffle.map(lambda x, y: model(x)).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "QNejnl8OvbA_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index = test_full_df.iloc[0].name\n",
    "\n",
    "before = np.full(INPUT_WIDTH, 0.0)\n",
    "after = np.fromiter(dataset.as_numpy_iterator(), np.float32)\n",
    "\n",
    "position_prediction_df = pd.DataFrame(data=np.concatenate((before, after), axis=0), columns=['position'], index=test_full_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "oyl4hhCnvl8d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_with_prediction = pd.concat([position_prediction_df, test_full_df.drop(columns=['position'])], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "PYGucvT_mS66",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16920/2657019458.py:179: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def open_order(self, index: int, open_datetime: np.str):\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "from pandas import DataFrame\n",
    "class Strategy:\n",
    "    fee = 0.2\n",
    "    # ограничение на максимальное количество открытых сделок\n",
    "    max_simultaneous_open_trades_limit = 1\n",
    "    # максимальное количество сделок которое было открыто одновременно\n",
    "    max_simultaneous_open_trades_count = 0\n",
    "\n",
    "    def __init__(self, max_simultaneous_open_trades_limit=1, start_balance=1000):\n",
    "        self.orders = pd.DataFrame(None,\n",
    "                                   columns=['type', 'open_price', 'open_datetime', 'close_price', 'close_datetime',\n",
    "                                            'profit', 'quantity', 'profit_as_a_percentage', 'take_profit_percentage',\n",
    "                                            'stop_loss_percentage', 'take_profit_price', 'stop_loss_price'],\n",
    "                                   ).astype({\n",
    "            'type': str, 'open_price': np.float32, 'open_datetime': str, 'close_price': np.float32,\n",
    "            'close_datetime': str,\n",
    "            'profit': np.float32, 'quantity': np.float32,\n",
    "            'profit_as_a_percentage': np.float32, 'take_profit_percentage': np.float32,\n",
    "            'stop_loss_percentage': np.float32, 'take_profit_price': np.float32, 'stop_loss_price': np.float32\n",
    "        })\n",
    "\n",
    "        self.max_simultaneous_open_trades_limit = max_simultaneous_open_trades_limit\n",
    "        self.start_balance = start_balance\n",
    "        self.balance = start_balance\n",
    "\n",
    "    def append_indicators(self, df: DataFrame) -> DataFrame:\n",
    "        return df\n",
    "\n",
    "    def get_orders(self):\n",
    "        return self.orders\n",
    "\n",
    "    def put_order(self, type: str, price: float, quantity: float, datetime: str = None,\n",
    "                  close_price: float = None, close_datetime: str = None,\n",
    "                  take_profit_percentage: float = None,\n",
    "                  stop_loss_percentage: float = None) -> int:\n",
    "\n",
    "        row = {\n",
    "            'type': type,\n",
    "            'open_price': price,\n",
    "            'open_datetime': datetime,\n",
    "            'close_price': close_price,\n",
    "            'close_datetime': close_datetime,\n",
    "            'profit': None,\n",
    "            'quantity': quantity,\n",
    "            'profit_as_a_percentage': None,\n",
    "            'take_profit_percentage': take_profit_percentage,\n",
    "            'stop_loss_percentage': stop_loss_percentage,\n",
    "            'take_profit_price': None,\n",
    "            'stop_loss_price': None\n",
    "        }\n",
    "\n",
    "        if take_profit_percentage is not None:\n",
    "            row['take_profit_price'] = (row['take_profit_percentage'] + 100) * price / 100\n",
    "\n",
    "        if stop_loss_percentage is not None:\n",
    "            row['stop_loss_price'] = (100 - row['stop_loss_percentage']) * price / 100\n",
    "\n",
    "        self.orders.loc[self.orders.shape[0]] = row\n",
    "\n",
    "        open_trades_count = len(self.get_open_orders())\n",
    "\n",
    "        if open_trades_count > self.max_simultaneous_open_trades_count:\n",
    "            self.max_simultaneous_open_trades_count = open_trades_count\n",
    "\n",
    "        self.balance = self.balance - quantity - quantity * self.fee / 100\n",
    "\n",
    "        return self.orders.iloc[-1].name\n",
    "\n",
    "    def is_order_closed(self, index: int):\n",
    "        if self.orders.at[index, 'close_price'] or self.orders.at[index, 'close_datetime']:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def close_order(self, index: int, price: float, datetime: str) -> bool:\n",
    "\n",
    "        index = int(index)\n",
    "\n",
    "        if self.is_order_closed(index):\n",
    "            return False\n",
    "\n",
    "        self.orders.at[index, 'close_price'] = float(price)\n",
    "        self.orders.at[index, 'close_datetime'] = str(datetime)\n",
    "\n",
    "        order = self.orders.iloc[index]\n",
    "\n",
    "        fee_for_opening_trade = order['quantity'] * self.fee / 100\n",
    "        fee_for_closing_trade = order['quantity'] * self.fee / 100\n",
    "\n",
    "        if order['type'] == 'long':\n",
    "\n",
    "            profit_as_a_percentage_without_commission = order['close_price'] * 100 / order['open_price'] - 100\n",
    "\n",
    "            amount_on_close_trade = (order['quantity'] * (100 + profit_as_a_percentage_without_commission) / 100)\n",
    "\n",
    "            self.balance += amount_on_close_trade - fee_for_closing_trade\n",
    "\n",
    "            self.orders.at[index, 'profit'] = amount_on_close_trade - order['quantity'] - fee_for_opening_trade - fee_for_closing_trade\n",
    "            self.orders.at[index, 'profit_as_a_percentage'] = profit_as_a_percentage_without_commission - self.fee * 2\n",
    "\n",
    "        elif order['type'] == 'short':\n",
    "\n",
    "            profit_as_a_percentage_without_commission = 100 - order['close_price'] * 100 / order['open_price']\n",
    "\n",
    "            amount_on_close_trade = (order['quantity'] * (100 + profit_as_a_percentage_without_commission) / 100)\n",
    "\n",
    "            self.balance += amount_on_close_trade - fee_for_closing_trade\n",
    "\n",
    "            self.orders.at[index, 'profit'] = amount_on_close_trade - order['quantity'] - fee_for_opening_trade \\\n",
    "                                              - fee_for_closing_trade\n",
    "            self.orders.at[index, 'profit_as_a_percentage'] = profit_as_a_percentage_without_commission - self.fee * 2\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_open_orders(self) -> DataFrame:\n",
    "        return self.orders[(self.orders['close_price'].isna()) & (pd.notna(self.orders['open_datetime']))]\n",
    "\n",
    "    def get_closed_orders(self) -> DataFrame:\n",
    "        return self.orders[pd.notna(self.orders['close_price'])]\n",
    "\n",
    "    def get_profit(self) -> float:\n",
    "        return self.orders['profit'].sum()\n",
    "\n",
    "    def get_profit_as_a_percentage(self) -> float:\n",
    "        return self.orders['profit_as_a_percentage'].sum()\n",
    "\n",
    "    def get_opened_orders_count(self) -> int:\n",
    "        return len(self.get_open_orders())\n",
    "\n",
    "    def get_closed_orders_count(self) -> int:\n",
    "        return len(self.get_closed_orders())\n",
    "\n",
    "    def get_win_trades(self) -> DataFrame:\n",
    "        closed = self.get_closed_orders()\n",
    "        return closed[closed['profit_as_a_percentage'] > 0]\n",
    "\n",
    "    def get_loss_trades(self) -> DataFrame:\n",
    "        closed = self.get_closed_orders()\n",
    "        return closed[closed['profit_as_a_percentage'] < 0]\n",
    "\n",
    "    def get_win_ration(self) -> float:\n",
    "        return 100 * len(self.get_win_trades()) / self.get_trades_count()\n",
    "\n",
    "    def get_trades_count(self) -> int:\n",
    "        return self.get_closed_orders_count()\n",
    "\n",
    "    def has_opened_orders(self) -> bool:\n",
    "        return len(self.get_open_orders()) > 0\n",
    "\n",
    "    def get_max_simultaneous_open_trades_count(self):\n",
    "        return self.max_simultaneous_open_trades_count\n",
    "\n",
    "    def get_pending_orders(self):\n",
    "        return self.orders[(self.orders['open_datetime'].isna()) & (pd.notna(self.orders['open_price']))]\n",
    "\n",
    "    def tick(self, candlestick):\n",
    "\n",
    "        open_trades = self.get_open_orders()\n",
    "\n",
    "        if len(open_trades) > 0:\n",
    "\n",
    "            for index, item in open_trades[candlestick['low'] < open_trades['stop_loss_price']].iterrows():\n",
    "                self.close_order(index, item['stop_loss_price'], candlestick['datetime'])\n",
    "\n",
    "            for index, item in open_trades[candlestick['high'] > open_trades['take_profit_price']].iterrows():\n",
    "                self.close_order(index, item['take_profit_price'], candlestick['datetime'])\n",
    "\n",
    "        pending_orders = self.get_pending_orders()\n",
    "\n",
    "        if len(pending_orders) > 0:\n",
    "\n",
    "            long_pending_orders = pending_orders[pending_orders['type'] == 'long']\n",
    "\n",
    "            for index, item in long_pending_orders[(candlestick['low'] < long_pending_orders['open_price']) & (\n",
    "                    candlestick['high'] > long_pending_orders['open_price'])].iterrows():\n",
    "                self.open_order(index, candlestick['datetime'])\n",
    "\n",
    "    def open_order(self, index: int, open_datetime: np.str):\n",
    "        self.orders.at[index, 'open_datetime'] = open_datetime\n",
    "\n",
    "    def backtest(self, df: DataFrame):\n",
    "        pbar = tqdm(df.iterrows(), total=len(df), colour='green')\n",
    "\n",
    "        for index, candlestick in pbar:\n",
    "            self.tick(candlestick=candlestick)\n",
    "            pbar.set_description(\n",
    "                f\"Strategy backtesting {index} Balance: {self.get_available_balance()} Open trades: {len(self.get_open_orders())} \")\n",
    "\n",
    "    def close_all_open_trades(self, candlestick):\n",
    "\n",
    "        open_trades = self.get_open_orders()\n",
    "\n",
    "        for index, item in open_trades.iterrows():\n",
    "            self.close_order(index, candlestick['close'], candlestick['datetime'])\n",
    "\n",
    "        pending_trades = self.get_pending_orders()\n",
    "\n",
    "        for index, item in pending_trades.iterrows():\n",
    "            self.cancel_order(index)\n",
    "\n",
    "    def get_available_balance(self) -> float:\n",
    "        return self.balance\n",
    "\n",
    "    def get_start_balance(self) -> float:\n",
    "        return self.start_balance\n",
    "\n",
    "    def cancel_order(self, index: int) -> bool:\n",
    "\n",
    "        index = int(index)\n",
    "\n",
    "        self.orders.drop([index], inplace=True)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "WNCqm5ki9PTy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30937936902046204 0.30009798794984816\n"
     ]
    }
   ],
   "source": [
    "MAX_POSITION = dataframe_with_prediction['position'].max()\n",
    "LIMIT = 97 * MAX_POSITION / 100\n",
    "\n",
    "print(MAX_POSITION, LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "I6GR99b8miKy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMStrategy(Strategy):\n",
    "    fee = 0.1\n",
    "    order_id = None\n",
    "\n",
    "    def append_indicators(self, df: DataFrame) -> DataFrame:\n",
    "        #df.ta.rsi(length=self.rsi, append=True)\n",
    "\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        df = (df.replace((np.inf, -np.inf), np.nan).dropna())\n",
    "\n",
    "        return df\n",
    "\n",
    "    def tick(self, candlestick):\n",
    "        super().tick(candlestick)\n",
    "\n",
    "        if self.get_opened_orders_count() < 1:\n",
    "          if candlestick['position'] > LIMIT:\n",
    "              self.order_id = self.put_order(\n",
    "                  type='long',\n",
    "                  price=candlestick['close'],\n",
    "                  datetime=candlestick['datetime'],\n",
    "                  quantity=500,\n",
    "                  take_profit_percentage=TAKE_PROFIT_PERCENT,\n",
    "                  stop_loss_percentage=STOP_LOSS_PERCENT\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "d6gZULRCmzBB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "strategy = LSTMStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "lEvl7HYtm3Ax",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Strategy backtesting 1440000 Balance: 1009.0 Open trades: 0 : 100%|\u001B[32m██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████\u001B[0m| 10000/10000 [00:32<00:00, 308.58it/s]\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "strategy.backtest(dataframe_with_prediction)\n",
    "strategy.close_all_open_trades(dataframe_with_prediction.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "DSWig8CJm4NR",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   type  open_price        open_datetime close_price       close_datetime  \\\n0  long     30099.5  2022-06-08 03:32:00    30701.49  2022-06-08 13:49:00   \n\n  profit  quantity profit_as_a_percentage  take_profit_percentage  \\\n0    9.0     500.0                    1.8                     2.0   \n\n   stop_loss_percentage  take_profit_price  stop_loss_price  \n0                   1.0           30701.49        29798.505  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>open_price</th>\n      <th>open_datetime</th>\n      <th>close_price</th>\n      <th>close_datetime</th>\n      <th>profit</th>\n      <th>quantity</th>\n      <th>profit_as_a_percentage</th>\n      <th>take_profit_percentage</th>\n      <th>stop_loss_percentage</th>\n      <th>take_profit_price</th>\n      <th>stop_loss_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>long</td>\n      <td>30099.5</td>\n      <td>2022-06-08 03:32:00</td>\n      <td>30701.49</td>\n      <td>2022-06-08 13:49:00</td>\n      <td>9.0</td>\n      <td>500.0</td>\n      <td>1.8</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>30701.49</td>\n      <td>29798.505</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.get_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M24Yz03vaU0R",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Результаты тестирования стратегии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "zphEeVNam7lR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start balance: 1000\n",
      "Available balance: 1009.0\n",
      "Profit as percentage: 1.8\n",
      "Profit: 9.0\n",
      "Win ratio: 100.0\n",
      "Max simultaneous trades count: 1\n"
     ]
    }
   ],
   "source": [
    "if len(strategy.get_orders()):\n",
    "  print(f'Start balance: {strategy.get_start_balance()}')\n",
    "  print(f'Available balance: {strategy.get_available_balance()}')\n",
    "  print(f'Profit as percentage: {strategy.get_profit_as_a_percentage()}')\n",
    "  print(f'Profit: {strategy.get_profit()}')\n",
    "  print(f'Win ratio: {strategy.get_win_ration()}')\n",
    "  print(f'Max simultaneous trades count: {strategy.get_max_simultaneous_open_trades_count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Take Profit and Stop Loss prediction training DATA LEAK",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}